{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wedingdong/sentiment-analysis-chatgpt-indobert-tfidf/blob/main/UAS_NLP_TF_IDF_KO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tahap 1: Setup & Impor **Library**"
      ],
      "metadata": {
        "id": "ormg0YC2BTpS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N1uxWEC_xRs"
      },
      "outputs": [],
      "source": [
        "#MENGINSTALL LIBRARY YANG DIBUTUHKAN\n",
        "!pip install scikit-learn Sastrawi pandas gradio --quiet\n",
        "\n",
        "#MELAKUKAN IMPORT\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import gradio as gr\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tahap 2: Load Data Mentah"
      ],
      "metadata": {
        "id": "7Nn_wQ8OBcTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD DATASET DARI GOOGLE DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "FILEPATH = \"/content/drive/MyDrive/UAS_NLP/Datasets_ulasan_chatgpt_BIndo.csv\"\n",
        "assert os.path.exists(FILEPATH), \"File tidak ditemukan!\"\n",
        "\n",
        "df = pd.read_csv(FILEPATH, on_bad_lines='skip', encoding='latin1')\n",
        "df.dropna(subset=['content','score'], inplace=True)\n",
        "print(\"Jumlah data setelah cleaning:\", len(df))\n",
        "\n",
        "def is_valid_score(x):\n",
        "    try:\n",
        "        if len(str(x)) > 5: return False\n",
        "        float(x)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "df = df[df['score'].apply(is_valid_score)].copy()\n",
        "# KONVERSI 'score' ke tipe data numerik\n",
        "df['score'] = pd.to_numeric(df['score'])\n",
        "\n",
        "print(\"Jumlah data setelah filtering skor:\", len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQBH-HhlBcs8",
        "outputId": "e0b01cd4-b53b-4950-e089-5b7a82800655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Jumlah data setelah cleaning: 13197\n",
            "Jumlah data setelah filtering skor: 13194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Tahap 3: Labeling & Balancing"
      ],
      "metadata": {
        "id": "3XHaWPtLBwlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KAMUS SENTIMEN (Definisi ulang untuk digunakan di hybrid_label)\n",
        "positive_keywords = [\"bagus\",\"keren\",\"mantap\",\"membantu\",\"top\",\"suka\",\"memuaskan\", \"works well\",\"mantab\",\"terbaik\",\"baik\",\"luar biasa\",\"recommended\", \"cepat\",\"responsive\",\"berguna\",\"helpful\",\"menyenangkan\"]\n",
        "negative_keywords = [\"buruk\",\"jelek\",\"error\",\"eror\",\"lemot\",\"lag\",\"kecewa\",\"gak bisa\", \"gabisa\",\"tidak bisa\",\"parah\",\"tidak puas\",\"masalah\",\"sampah\", \"payah\",\"crash\",\"hang\",\"ngefreeze\",\"mengecewakan\",\"tidak berfungsi\"]\n",
        "neutral_keywords = [\"biasa saja\",\"biasa aja\",\"lumayan\",\"cukup\",\"oke lah\",\"standar\",\"normal\", \"ya begitu\",\"ya gitu\",\"so-so\",\"biasa\",\"tidak terlalu\",\"netral\",\"oke\"]\n",
        "\n",
        "# PREPROCESS TEKS RINGAN\n",
        "def clean_text_hybrid(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^a-z0-9 ]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# FUNGSI HYBRID LABELING UTAMA\n",
        "def clean_text_hybrid(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^a-z0-9 ]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def hybrid_label(content, score):\n",
        "    # Mapping Skor\n",
        "    if score <= 2: label_score = 0  # Negatif\n",
        "    elif score == 3: label_score = 1  # Netral\n",
        "    else: label_score = 2  # Positif\n",
        "\n",
        "    # Labeling Kamus\n",
        "    t = clean_text_hybrid(content)\n",
        "    pos = sum(k in t for k in positive_keywords)\n",
        "    neg = sum(k in t for k in negative_keywords)\n",
        "\n",
        "    # Aturan Konflik: Positif + Negatif = Netral\n",
        "    if pos > 0 and neg > 0:\n",
        "        return 1\n",
        "\n",
        "    # Jika tidak ada konflik, ikuti Mapping Skor\n",
        "    return label_score\n",
        "\n",
        "# --- Terapkan Hybrid Labeling (Koreksi Cara Apply) ---\n",
        "# MEMANGGIL FUNGSI DENGAN MENGIRIMKAN SELURUH BARIS (axis=1)\n",
        "df['label'] = df.apply(lambda row: hybrid_label(row['content'], row['score']), axis=1)\n",
        "\n",
        "df_clean = df[['content', 'label']].copy()\n",
        "\n",
        "print(\"Distribusi label sebelum balancing:\")\n",
        "print(df_clean['label'].value_counts())\n",
        "\n",
        "# Oversampling balancing agar setiap kelas sama banyak datanya\n",
        "target_count = df_clean['label'].value_counts().max()\n",
        "\n",
        "df_resampled = pd.concat([\n",
        "    df_clean[df_clean['label'] == k]\n",
        "    if len(df_clean[df_clean['label'] == k]) == target_count\n",
        "    else resample(\n",
        "        df_clean[df_clean['label'] == k],\n",
        "        replace=True,\n",
        "        n_samples=target_count,\n",
        "        random_state=42\n",
        "    )\n",
        "    for k in df_clean['label'].unique()\n",
        "], ignore_index=True).sample(frac=1, random_state=42)\n",
        "\n",
        "print(\"Distribusi label setelah balancing:\")\n",
        "print(df_resampled['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1WVr35FBxac",
        "outputId": "3034eb10-5504-48d2-f657-8c9aec09e1ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribusi label sebelum balancing:\n",
            "label\n",
            "2    12308\n",
            "1      450\n",
            "0      436\n",
            "Name: count, dtype: int64\n",
            "Distribusi label setelah balancing:\n",
            "label\n",
            "1    12308\n",
            "0    12308\n",
            "2    12308\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tahap 4: Pra-Prosesing Teks (Cleaning, Stopword, Stemming)\n",
        "\n"
      ],
      "metadata": {
        "id": "cKj2GBJiCGVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "factory_stop = StopWordRemoverFactory()\n",
        "\n",
        "stopwords = set(factory_stop.get_stop_words()) - {\"tidak\", \"kurang\", \"tanpa\", \"belum\"}\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    # Penghapusan URL, Angka, Simbol\n",
        "    text = re.sub(r'https?://\\\\S+|www\\\\.\\\\S+', ' ', text)\n",
        "    text = re.sub(r'\\\\d+', ' ', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\\\s]', ' ', text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Stopword Removal\n",
        "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
        "    # Stemming\n",
        "    text = stemmer.stem(text)\n",
        "\n",
        "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df_resampled['content_clean'] = df_resampled['content'].apply(clean_text)"
      ],
      "metadata": {
        "id": "WZtYCQ-zCGod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tahap 5: Train-Test Split"
      ],
      "metadata": {
        "id": "OfYtD7zXJrv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_resampled['content_clean'].values\n",
        "y = df_resampled['label'].values\n",
        "# Pisahkan fitur teks bersih dan label sentimen.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "# Bagi data jadi train:test 80:20,"
      ],
      "metadata": {
        "id": "Y0Vf74bfJpvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tahap 6: TF-IDF Vectorization"
      ],
      "metadata": {
        "id": "Saoa25XXJtUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(max_features=6000, ngram_range=(1,2))\n",
        "X_train_vec = tfidf.fit_transform(X_train)  #Melakukan training\n",
        "X_test_vec = tfidf.transform(X_test)        #Melakukan Testing\n"
      ],
      "metadata": {
        "id": "lA5mjIVMJtnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tahap 7: Training Model ML dengan smoothing"
      ],
      "metadata": {
        "id": "pOlvfEtcJ_JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# Training model TF-IDF menggunakan Naive Bayes dengan smoothing\n",
        "clf_nb = MultinomialNB(alpha=1.0)\n",
        "clf_nb.fit(X_train_vec, y_train)\n",
        "\n",
        "# Menghasilkan prediksi untuk data testing\n",
        "y_pred_nb = clf_nb.predict(X_test_vec)\n",
        "\n",
        "print(\"=== Hasil Training TF IDF ===\")\n",
        "print(classification_report(y_test, y_pred_nb, target_names=[\"Negatif\", \"Netral\", \"Positif\"]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALoMYbC1KD_F",
        "outputId": "a25d1b60-9476-4996-ca1c-2829f2ad8658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Hasil Training TF IDF ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.88      0.87      0.87      2461\n",
            "      Netral       0.93      0.79      0.85      2462\n",
            "     Positif       0.73      0.85      0.79      2462\n",
            "\n",
            "    accuracy                           0.84      7385\n",
            "   macro avg       0.85      0.84      0.84      7385\n",
            "weighted avg       0.85      0.84      0.84      7385\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tahap 8: Evaluasi Model"
      ],
      "metadata": {
        "id": "c9IM3CzTJt20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghasilkan prediksi model untuk data testing\n",
        "y_pred = clf_nb.predict(X_test_vec)\n",
        "\n",
        "# --- Output Classification Report ---\n",
        "print(\"=== Laporan Klasifikasi Sentimen ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Negatif\", \"Netral\", \"Positif\"]))\n",
        "\n",
        "#  Menghitung dan Menampilkan Akurasi Keseluruhan\n",
        "akurasi = accuracy_score(y_test, y_pred)\n",
        "print(f\"Akurasi Keseluruhan: {akurasi * 100:.2f}%\")\n",
        "\n",
        "# Menghitung dan Menampilkan F1-Score Netral (Persen)\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Menghitung F1-score untuk setiap kelas (Negatif, Netral, Positif)\n",
        "# Diasumsikan 0: Negatif, 1: Netral, 2: Positif\n",
        "f1_per_kelas = f1_score(y_test, y_pred, average=None)\n",
        "\n",
        "# F1-Score Netral berada pada indeks 1\n",
        "# Mengalikan dengan 100 dan memformat ke 2 desimal\n",
        "f1_netral = f1_per_kelas[1]\n",
        "print(f\"F1-Score Netral: {f1_netral * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7satqKZJyNs",
        "outputId": "dc9035f9-a01e-4838-97ff-f43420f51d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Laporan Klasifikasi Sentimen ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.88      0.87      0.87      2461\n",
            "      Netral       0.93      0.79      0.85      2462\n",
            "     Positif       0.73      0.85      0.79      2462\n",
            "\n",
            "    accuracy                           0.84      7385\n",
            "   macro avg       0.85      0.84      0.84      7385\n",
            "weighted avg       0.85      0.84      0.84      7385\n",
            "\n",
            "Akurasi Keseluruhan: 83.55%\n",
            "F1-Score Netral: 85.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tahap 9: Deploy Gradio UI"
      ],
      "metadata": {
        "id": "npCkCfecJzyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {0: \"Negatif\", 1: \"Netral\", 2: \"Positif\"}\n",
        "\n",
        "# KATA KUNCI POSITIF NEGATIF\n",
        "positive_keywords = [\"bagus\",\"keren\",\"mantap\",\"membantu\",\"top\",\"suka\",\"memuaskan\", \"kerja bagus\",\"mantab\",\"terbaik\",\"baik\",\"luar biasa\",\"recommended\", \"cepat\",\"responsive\",\"berguna\",\"helpful\",\"menyenangkan\",\"tidak jelek\",\"tidak buruk\"]\n",
        "negative_keywords = [\"buruk\",\"jelek\",\"error\",\"eror\",\"lemot\",\"lag\",\"kecewa\",\"gak bisa\", \"gabisa\",\"tidak bisa\",\"parah\",\"tidak puas\",\"masalah\",\"sampah\", \"payah\",\"crash\",\"hang\",\"ngefreeze\",\"mengecewakan\",\"tidak berfungsi\",\"tidak bagus\",\"todak suka\"]\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    if not text or str(text).strip() == \"\":\n",
        "        return \"ERROR: Masukkan teks ulasan terlebih dahulu.\"\n",
        "\n",
        "    # 1. Pra-proses teks untuk MENDETEKSI KAMUS (Cukup clean_text_hybrid yang ringan)\n",
        "    cleaned_hybrid = clean_text_hybrid(text)\n",
        "    pos_count = sum(k in cleaned_hybrid for k in positive_keywords)\n",
        "    neg_count = sum(k in cleaned_hybrid for k in negative_keywords)\n",
        "\n",
        "    #  ATURAN HYBRID OVERRIDE\n",
        "    if pos_count > 0 and neg_count > 0:\n",
        "        # Konflik Positif + Negatif = Netral (skor 1)\n",
        "        pred_score = 1\n",
        "        pred_label = label_map[pred_score]\n",
        "        return f\"Hasil: {pred_label} (Skor: {pred_score})\"\n",
        "\n",
        "    # JIKA TIDAK ADA KONFLIK, GUNAKAN MODEL ML\n",
        "    try:\n",
        "        # 2. Pra-proses teks Penuh (untuk TF-IDF)\n",
        "\n",
        "        cleaned_full = clean_text(text)\n",
        "\n",
        "        # 3. Vektorisasi & Prediksi Model ML\n",
        "        vec = tfidf.transform([cleaned_full])\n",
        "        pred_score = clf_nb.predict(vec)[0]\n",
        "        pred_label = label_map[pred_score]\n",
        "\n",
        "        return f\"Hasil: {pred_label} (Skor: {pred_score})\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"ERROR Pemrosesan: Terjadi kesalahan. Pastikan semua tahap di-run. ({e})\"\n",
        "\n",
        "\n",
        "# Membangun Antarmuka Gradio\n",
        "iface = gr.Interface(\n",
        "    fn=predict_sentiment,\n",
        "    inputs=gr.Textbox(lines=3, placeholder=\"Tulis ulasan...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Analisis Sentimen (TF-IDF)\",\n",
        "    description=\"Model menggabungkan TF-IDF dan logika kamus konflik (Positif+Negatif=Netral) dan prediksi Naive Bayes.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "LJvrPar4J0E7",
        "outputId": "bd734374-8735-473b-c245-70d2149a29cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://95f27b4ada645a76d7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://95f27b4ada645a76d7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6-t6NsCWeSA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}